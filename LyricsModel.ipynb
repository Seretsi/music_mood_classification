{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QiTTn5KPS3FC"
      },
      "outputs": [],
      "source": [
        "print(\"Installing packages...\")\n",
        "!pip install -q transformers torch pandas scikit-learn matplotlib tqdm\n",
        "print(\"Installed!\\n\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/MERGE_Lyrics_Complete'\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 30\n",
        "PATIENCE = 7\n",
        "DROPOUT_RATE = 0.3\n",
        "LEARNING_RATE_ROBERTA = 1e-5\n",
        "LEARNING_RATE_CUSTOM = 1e-3\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\\n\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(f\"\\nDrive mounted\")\n",
        "print(f\"Looking for dataset at: {DATASET_PATH}\")\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    print(\"Dataset not found! Update DATASET_PATH variable.\")\n",
        "    raise FileNotFoundError(f\"Cannot find {DATASET_PATH}\")\n",
        "\n",
        "print(f\"Dataset found!\\n\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "av_path = os.path.join(DATASET_PATH, 'merge_lyrics_complete_av_values.csv')\n",
        "av_df = pd.read_csv(av_path)\n",
        "print(f\"Loaded {len(av_df)} songs\")\n",
        "\n",
        "ΩΩΩprint(f\"Original ranges: Arousal [{av_df['Arousal'].min():.3f}, {av_df['Arousal'].max():.3f}], Valence [{av_df['Valence'].min():.3f}, {av_df['Valence'].max():.3f}]\")\n",
        "av_df['Arousal'] = 2 * av_df['Arousal'] - 1\n",
        "av_df['Valence'] = 2 * av_df['Valence'] - 1\n",
        "print(f\"Normalized ranges: Arousal [{av_df['Arousal'].min():.3f}, {av_df['Arousal'].max():.3f}], Valence [{av_df['Valence'].min():.3f}, {av_df['Valence'].max():.3f}]\")\n",
        "\n",
        "def read_lyrics(song_id, base_path):\n",
        "    for q in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
        "        path = os.path.join(base_path, q, f'{song_id}.txt')\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                return f.read().strip()\n",
        "    return None\n",
        "\n",
        "print(\"\\nReading lyrics files...\")\n",
        "data = []\n",
        "for _, row in tqdm(av_df.iterrows(), total=len(av_df)):\n",
        "    lyrics = read_lyrics(row['Song'], DATASET_PATH)\n",
        "    if lyrics and len(lyrics) > 10:\n",
        "        data.append({\n",
        "            'song_id': row['Song'],\n",
        "            'lyrics': lyrics,\n",
        "            'valence': row['Valence'],\n",
        "            'arousal': row['Arousal']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Loaded {len(df)} songs with lyrics\")\n",
        "print(f\"Verify normalization - Arousal: [{df['arousal'].min():.3f}, {df['arousal'].max():.3f}], Valence: [{df['valence'].min():.3f}, {df['valence'].max():.3f}]\\n\")\n",
        "\n",
        "splits_dir = os.path.join(DATASET_PATH, 'tvt_dataframes', 'tvt_70_15_15')\n",
        "train_ids = set(pd.read_csv(os.path.join(splits_dir, 'tvt_70_15_15_train_lyrics_complete.csv'))['Song'].values)\n",
        "val_ids = set(pd.read_csv(os.path.join(splits_dir, 'tvt_70_15_15_validate_lyrics_complete.csv'))['Song'].values)\n",
        "test_ids = set(pd.read_csv(os.path.join(splits_dir, 'tvt_70_15_15_test_lyrics_complete.csv'))['Song'].values)\n",
        "\n",
        "train_df = df[df['song_id'].isin(train_ids)].reset_index(drop=True)\n",
        "val_df = df[df['song_id'].isin(val_ids)].reset_index(drop=True)\n",
        "test_df = df[df['song_id'].isin(test_ids)].reset_index(drop=True)\n",
        "\n",
        "print(f\"Splits: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\\n\")\n",
        "\n",
        "class LyricsEmotionRegressor(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.embedding_projection = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, return_embeddings=False):\n",
        "        bert_out = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        features = bert_out.last_hidden_state[:, 0, :]\n",
        "\n",
        "        h = self.relu(self.fc1(features))\n",
        "        h = self.dropout(h)\n",
        "        h = self.relu(self.fc2(h))\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        embeddings = self.relu(self.embedding_projection(h))\n",
        "\n",
        "        output = self.tanh(self.fc3(embeddings))\n",
        "\n",
        "        return (output, embeddings) if return_embeddings else output\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "class MERGEDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.lyrics = dataframe['lyrics'].values\n",
        "        self.valence = dataframe['valence'].values\n",
        "        self.arousal = dataframe['arousal'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lyrics)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(\n",
        "            str(self.lyrics[idx]),\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target = torch.tensor([self.valence[idx], self.arousal[idx]], dtype=torch.float)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'target': target\n",
        "        }\n",
        "\n",
        "train_dataset = MERGEDataset(train_df)\n",
        "val_dataset = MERGEDataset(val_df)\n",
        "test_dataset = MERGEDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"DataLoaders ready: {len(train_loader)} train batches\\n\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CREATING MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = LyricsEmotionRegressor(dropout_rate=DROPOUT_RATE).to(device)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.roberta.parameters(), 'lr': LEARNING_RATE_ROBERTA},\n",
        "    {'params': model.fc1.parameters(), 'lr': LEARNING_RATE_CUSTOM},\n",
        "    {'params': model.fc2.parameters(), 'lr': LEARNING_RATE_CUSTOM},\n",
        "    {'params': model.embedding_projection.parameters(), 'lr': LEARNING_RATE_CUSTOM},\n",
        "    {'params': model.fc3.parameters(), 'lr': LEARNING_RATE_CUSTOM}\n",
        "])\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(\"Model and optimizer created\\n\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_losses, val_losses = [], []\n",
        "val_mae_v_list, val_mae_a_list = [], []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', leave=False):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "\n",
        "        predictions = model(input_ids, attention_mask)\n",
        "        loss = criterion(predictions, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds, val_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "\n",
        "            predictions = model(input_ids, attention_mask)\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_preds.append(predictions.cpu())\n",
        "            val_targets.append(targets.cpu())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    val_preds = torch.cat(val_preds)\n",
        "    val_targets = torch.cat(val_targets)\n",
        "    mae_v = torch.mean(torch.abs(val_preds[:, 0] - val_targets[:, 0])).item()\n",
        "    mae_a = torch.mean(torch.abs(val_preds[:, 1] - val_targets[:, 1])).item()\n",
        "    val_mae_v_list.append(mae_v)\n",
        "    val_mae_a_list.append(mae_a)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}, MAE V={mae_v:.4f} A={mae_a:.4f}', end='')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch + 1\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(' BEST')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print()\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f'\\nEarly stopping at epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "print(f'\\nLoaded best model from epoch {best_epoch}\\n')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].plot(train_losses, 'o-', label='Train')\n",
        "axes[0].plot(val_losses, 's-', label='Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Progress')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(val_mae_v_list, 'o-', label='Valence MAE')\n",
        "axes[1].plot(val_mae_a_list, 's-', label='Arousal MAE')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].set_title('Validation MAE')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.eval()\n",
        "test_preds, test_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc='Testing'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        targets = batch['target'].to(device)\n",
        "\n",
        "        predictions = model(input_ids, attention_mask)\n",
        "        test_preds.append(predictions.cpu())\n",
        "        test_targets.append(targets.cpu())\n",
        "\n",
        "test_preds = torch.cat(test_preds)\n",
        "test_targets = torch.cat(test_targets)\n",
        "\n",
        "test_loss = criterion(test_preds, test_targets).item()\n",
        "mae_v = torch.mean(torch.abs(test_preds[:, 0] - test_targets[:, 0])).item()\n",
        "mae_a = torch.mean(torch.abs(test_preds[:, 1] - test_targets[:, 1])).item()\n",
        "corr_v = np.corrcoef(test_preds[:, 0], test_targets[:, 0])[0, 1]\n",
        "corr_a = np.corrcoef(test_preds[:, 1], test_targets[:, 1])[0, 1]\n",
        "\n",
        "print(f\"\\nTest MSE: {test_loss:.4f}\")\n",
        "print(f\"Test MAE - Valence: {mae_v:.4f}, Arousal: {mae_a:.4f}\")\n",
        "print(f\"Correlation - Valence: {corr_v:.4f}, Arousal: {corr_a:.4f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "axes[0].scatter(test_targets[:, 0], test_preds[:, 0], alpha=0.5, s=20)\n",
        "axes[0].plot([-1, 1], [-1, 1], 'r--', lw=2)\n",
        "axes[0].set_xlabel('True Valence')\n",
        "axes[0].set_ylabel('Predicted Valence')\n",
        "axes[0].set_title(f'Valence (MAE={mae_v:.3f}, r={corr_v:.3f})')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].scatter(test_targets[:, 1], test_preds[:, 1], alpha=0.5, s=20)\n",
        "axes[1].plot([-1, 1], [-1, 1], 'r--', lw=2)\n",
        "axes[1].set_xlabel('True Arousal')\n",
        "axes[1].set_ylabel('Predicted Arousal')\n",
        "axes[1].set_title(f'Arousal (MAE={mae_a:.3f}, r={corr_a:.3f})')\n",
        "axes[1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTRACTING 128-DIM EMBEDDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"Extracting embeddings...\")\n",
        "\n",
        "model.eval()\n",
        "test_embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc='Embeddings'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        _, embeddings = model(input_ids, attention_mask, return_embeddings=True)\n",
        "        test_embeddings.append(embeddings.cpu().numpy())\n",
        "\n",
        "test_embeddings = np.concatenate(test_embeddings, axis=0)\n",
        "print(f\"\\nExtracted embeddings: {test_embeddings.shape}\")\n",
        "\n",
        "embeddings_df = pd.DataFrame({\n",
        "    'song_id': test_df['song_id'].values,\n",
        "    **{f'emb_{i}': test_embeddings[:, i] for i in range(128)},\n",
        "    'true_valence': test_targets[:, 0].numpy(),\n",
        "    'true_arousal': test_targets[:, 1].numpy(),\n",
        "    'pred_valence': test_preds[:, 0].numpy(),\n",
        "    'pred_arousal': test_preds[:, 1].numpy()\n",
        "})\n",
        "\n",
        "embeddings_df.to_csv('embeddings_128dim_with_metadata.csv', index=False)\n",
        "print(f\"Saved to 'embeddings_128dim_with_metadata.csv'\")\n",
        "\n",
        "np.save('test_embeddings_128dim.npy', test_embeddings)\n",
        "print(f\"Saved numpy array to 'test_embeddings_128dim.npy'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXAMPLE PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def predict(text):\n",
        "    model.eval()\n",
        "    enc = tokenizer(text, add_special_tokens=True, max_length=512,\n",
        "                   padding='max_length', truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        pred, emb = model(enc['input_ids'].to(device),\n",
        "                         enc['attention_mask'].to(device),\n",
        "                         return_embeddings=True)\n",
        "    v, a = pred[0, 0].item(), pred[0, 1].item()\n",
        "    return v, a, emb[0].cpu().numpy()\n",
        "\n",
        "examples = [\n",
        "    \"I'm so happy and excited, best day ever!\",\n",
        "    \"Feeling sad and lonely, crying alone\",\n",
        "    \"So angry I could explode with rage!\",\n",
        "    \"Peaceful and calm, enjoying the silence\",\n",
        "    \"Terrified and anxious, heart pounding\"\n",
        "]\n",
        "\n",
        "for text in examples:\n",
        "    v, a, emb = predict(text)\n",
        "    print(f\"\\n'{text[:50]}...'\")\n",
        "    print(f\"  Valence: {v:+.3f}, Arousal: {a:+.3f}\")\n",
        "    print(f\"  Embedding shape: {emb.shape} (first 5 values: {emb[:5]})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING FINAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'test_mae_valence': mae_v,\n",
        "    'test_mae_arousal': mae_a,\n",
        "    'best_epoch': best_epoch\n",
        "}, 'final_model.pth')\n",
        "\n",
        "print(\"Saved to 'final_model.pth'\")\n",
        "\n",
        "print(\"\\nDone\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}